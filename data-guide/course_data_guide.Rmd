---
title: "Course Demand & Coursetable Data Guide"
author: "Daniel Zhao"
date: "February 16, 2020"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
  rmarkdown::github_document:
    toc: yes
---

```{r, message = F}
knitr::opts_chunk$set(message = F)

library(dplyr)
library(readr)
library(ggplot2)
```


The purpose of this document is to outline existing work, as of February 16, 2010, pertaining to the following:

- How to scrape Yale Course Demand Statistics using a script we've written
- How this data is stored, and how to work with it
- How to scrape CourseTable
- Ways to merge the two, and some issues that arise in it
- Existing visualization and analysis regarding course data

# Scraping Course Demand

The script we use to scrape Yale Course Demand Statistics can be found at `/projects/courses/scrape_cds_v2.py`.

## Data source

Familiarize yourself with how the Course Demand data is provided by Yale by viewing them at https://ivy.yale.edu/course-stats/. Take note of a few things:

- Cross-listed courses appear once in each department in which a course is cross-listed
- Some courses have both an undergraduate and graduate listing in the same department
- Shopping period lasts approximately two weeks
- There's data for many departments that don't offer undergraduate courses, which means we're not interested

## Dependencies

The only dependency that you should install is `BeautifulSoup`, which is a Python library that parses HTML pages to make them easier to process. You can install the library by typing the following commands in your command line (Command Prompt on Windows, Terminal on Mac):
```
# if you're using Anaconda
conda install beautifulsoup4

# if you're using vanilla Python
pip install beautifulsoup4
```

## Running the script

You should be able to run the script from your preferred Python environment. That is, you can open the file in your preferred IDE (such as Visual Studio Code), run it from the command line, or, if you insist, copy-and-paste it into a Jupyter Notebook cell to run it there.

We can show you how to use either of the three, but because the script is fully ready-to-go, it's easiest to run it from the command line (if you're comfortable with that).

Instructions for Windows, assuming you've added `python` to your PATH variables:

- Open `cmd.exe`
- Enter `cd [path to the script]`
- Enter `python scrape_cds_v2.py` and wait for it to scrape!

There's an optional command-line argument to specify the semester you want to scrape. For example, to scrape Fall 2019, you should enter `scrape_cds_v2.py 201903` instead; see below for explanation semester codes.

The scraping should take a few minutes, depending on your system and internet connection; a recent scrape on my computer took 3 minutes, 9.6 seconds. The script will display its progress as it scrapes.

## Scraping specific semesters

By default, the script scrapes the Spring 2020 semester. You can set another semester to scrape by going to line 32 and changing the `semester` variable to the semester code for the semester for which you want to scrape.

The semester code is a six-digit number used by Yale, with the first four corresponding to the year and the last two corresponding to Spring (`01`), Summer (`02`), or Fall (`03`). For example:

- Spring 2020 would be `202001`
- Fall 2018 would be `201803`
- Spring 2017 would be `201701`

## How the data is stored

If you look at the data, you'll find three separate files instead of one. That seems unintuitive; why could we just have one file with all of the courses? The reasoning lies in several characteristics inherent to the data. In this section, I'll explain some of the choices I had to make in storing the data.

The simplest way to store the data would to use one row per course, and one column per day of shopping period. Thus, if a department had fifty courses offered, and shoppping period lasted for fourteen days, we would have a $50 \times 15$ table (fifteen because the first column is the name of the course).

This presents several issues:

- **What do we do if a course is cross-listed?** We don't want the same course to be listed twice in the table, because then visualizations would have duplicate data points. So we want to make sure that a unique given course occupies only one row in the table.
- **How can we make sure that we can filter courses by department?** This problem would be easy if courses weren't cross-listed, because we can just create a new column with the name of the department. But since we decided that one unique course should only occupy one row in the data no matter how many times it's cross-listed, which department should this new column contain?
- **How can we make it easier to visualize the data?** While the one-row-per-course, one-column-per-day format is intuitive an easy to conceptualize, it's not in the spirit of how most modern data visualization tools intake their data. For example, both `ggplot2` in R and D3.js, the JavaScript library used to create interactive online visualizations, prefer one row *per data point*. What constitutes a data point? When you make a line chart of course enrollment over fourteen days, ``ggplot2` actually thinks it's plotting *fourteen* data points (one at each day), which means that `ggplot2` is looking for fourteen rows of data, not one row with fourteen columns.

(For those curious: these issues point pretty naturally to storing data in *tidy form* or *long form*, instead of *wide form*. More generally, we're dancing around the issue of *database normalization*, which you might have encountered if you've ever used SQL. You can read more about tidy data [here](https://r4ds.had.co.nz/tidy-data.html).)

The solution is to these issues is to split the data out into three tables:

- `courses.csv`
- `demand.csv`
- `subjects.csv`

```{r}
courses <- read_csv("courses.csv")
demand <- read_csv("demand.csv")
subjects <- read_csv("subjects.csv")
```


### `courses.csv`

Let's start with `courses.csv`, which contains *one row per course code*:

```{r}
courses
```

You'll see that:

- Each *unique course code* gets one row in the table, even courses that are cross-listed
- When courses are cross-listed, they get separate rows for each course code, but they share the `name`
- When courses are cross-listed, all listings for the same course get the same `id`

### `demand.csv`

This file contains the bulk of the demand data. Here's what it looks like:

```{r}
demand[1:10, ]
```

Here's another section of the data:

```{r}
demand[1236:1245, ]
```

Do you see what's happening? `demand` contains one row per *day* of shopping period, which means that for each course, there are 20 rows, since there are twenty days in shopping period. A consequence is that this makes it super easy to use many visualization tools:

```{r}
demand %>%
  filter(id %in% 1:20) %>%
  ggplot(aes(x = date, y = count, group = id)) +
  geom_point() + geom_line()
```

If you're familiar with `ggplot`, what's happening is this: first, it plots a point at every (`date`, `count`) observation in the table. Then, it connects all the dots that have the same `id` variable. This wouldn't easily be possible had we stored our data in *wide* format, with one row per course and one column per day of shopping period, and it'd require some more intensive data reshaping. This format might seem uninituitive at first, but it makes plotting in `ggplot` and in D3.js (the online visualization tol) much easier that it's worth the additional upfront complexity.

What's the `id` variable doing here, and what does the `id` correspond to? Turns out that this `id` variable is the same as the `id` in the `courses` table we looked at earlier! So `id = 1` corresponds to ACCT 270, Foundations of Accounting & Valuation from the first table above.

### Merging `courses` and `demand`

At this point you might ask how we can match courses from the `demand` table to those in the `courses` table. Those familiar with database operations will know that we'll use a `join`, whereby we match rows in one table to rows in another. Here's a basic example, using the `left_join()` function in `dplyr`:

- We take the first table (known as the "left table"), which is `courses`
- Then we look at the second table (known as the "right table"), which is `demand`
- Using any columns in common, which in this case is `id`, we match rows, duplicating rows as necessary

Note that last point: in the left table, we used to have one row per `id`, but in the right table, we had twenty rows per `id`, so the `left_join()` operation duplicated the rows on the left until we have one row per `date` of shopping period.

```{r}
left_join(courses, demand)
```

For those familiar with database operations and SQL, these are your typical SQL joins; we'll mostly be using left joins and inner joins. You'll also recognize that the considerations we talked about earlier are just a matter of achieving somewhere around third normal form, or 3NF. (No worries if that doesn't mean anything to you!)

**For those unfamiliar with joins, you should read up on [relational data and joins here](https://r4ds.had.co.nz/relational-data.html). We'll use joins a lot for the YDN Data Desk, not just for the `demand` and `courses` tables but also to merge in CourseTable data, so it's a good idea to wrap your head around what the computer is doing when you tell it to join rows.** Joins aren't just useful for course-related datasets: a lot of the Data Desk's work will involve combining disparate datasets, so being comfortable joins will be an important skill.

At this point, it might also be clearer how we've dealt with the three issues we described at the beginning of this section. Here were the three issues, and here's how we've dealt with them:

- **Cross-listing of courses**: to avoid writing duplicate data, the `demand` table only contains one set of numbers for each unique courses, no matter how often it is cross-listed
- **Filtering courses by department**: we can start with the `courses` table, extract the department code from the entire course code (e.g. extracting "AFAM" from "AFAM 270"), then filter from there. Since courses that are cross-listed get separate rows, we can filter by one department and ensure that we've still captured all the courses that were ever listed in that department
- **Easy visualization**: as we demonstrated above, the one-row-per-day format makes this easy

### `subjects.csv`

The last table we have is fairly straightforward: it simply includes a list of the department codes, and the full names of those departments:

```{r}
subjects
```

## Working with the data

I tend to use packages from the tidyverse family, so I'll load those below. (I could just call `library(tidyverse)` once, although I tend to avoid doing that in order to minimize cases where functions of the same name conflict with each other one package will override functions from the other.)

Side note: the **tidyverse** is a set of R packages that streamline common data manipulation tasks.Not all R users use the tidyverse, and you can do anything in the tidyverse without using the tidyverse; using the core R functionality instead of tidyverse functionality is called **Base R**. For YDN Data, you can expect to see code written in both using Base R and in tidyverse, and you can feel free to use whichever you're more comfortable with.

```{r}
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
```

Reading in the files, which is the same as what we did above:

```{r}
courses <- read_csv("courses.csv")
demand <- read_csv("demand.csv")
subjects <- read_csv("subjects.csv")
```

Below, the code that I include will use the `%>%` operator, known as the pipe. If you're not familiar with the pipe, you should skim through [this page](https://r4ds.had.co.nz/pipes.html) to get a basic understanding of what it is and what it does. You don't have to write code using the pipe, but it comes up frequently enough when you search for help with R code online (such as on Stack Overflow) that it's helpful to know what it does.

I also use several functions from `dplyr`, a package designed for data analysis and manipulation. For now, it suffices to know that there are a few primary operations from the `dplyr` package:

- `mutate(df, newCol = oldCol + 2)` creates a new column in `df` called `newCol` that adds 2 to `oldCol`
- `filter(df, oldCol == 2)` filters `df` to only those rows where `oldCol` equals 2
- `select(df, oldCol, newCol, newNewCol)` extracts only those three columns in `df`
- `arrange(df, desc(oldCol))` orders the rows in `df` in descending order of `oldCol`
- Finally, you'll often see `group_by(df, category)` used in conjunction with `summarize(df, meanPerGroup = mean(var))`. This chain calculates the mean of `var` for each `category`.

At some point, it might be helpful to read or skim through [this page](https://r4ds.had.co.nz/transform.html), which is an introduction to data transformation using `dplyr`, although if the code below doesn't seem too difficult to understand, you probably don't need to bother.

Let's get to manipulating the data!

### Dealing with course codes

You'll find that the `code` column in the `courses` table is actually a character vector, rather than a numeric. That's because it contains the subject code, but also because some courses have an additional letter on the end, such as "L" for labs and "J" for history seminars. These functions help us work deal with them; you can feel free to do this cleaning on your own, or just copy/paste and call these functions:

```{r}
unite_code <- function(df) {
	tidyr::unite(df, code, subject, number, sep = " ", remove = FALSE)
}

separate_code <- function(df) {
	tidyr::separate(df, code, into = c("subject", "number"), sep = " ", remove = FALSE)
}

separate_designator <- function(df) {
	num <- df$number
	
	df$number <- as.numeric(str_extract(num, "[[:digit:]]+"))
	df$designator <- str_extract(num, "[:alpha:]")
	
	df
}
```

You can call these functions on the `number` column like so:

```{r}
courses <- courses %>% 
  separate_code() %>%
  separate_designator()

courses
```

Notice that now, the `number` column is a numeric variable, which it makes it easy to filter courses by course code. For example, we can filter for labs by looking for "L" in the designator column:

```{r}
courses %>%
  filter(designator == "L")
```

Freshman seminars are numbered 099 and below, so you can filter for freshman seminars using:

```{r}
courses %>%
  filter(number < 100)
```

### Filtering and graphing: example

Let's take this further: let's plot the demand of all freshman seminars over the course of shopping period. We'll:

- Filter for course codes less than 100
- Take courses with distinct `id`'s, since cross-listed courses are listed more than once
- Merge in demand data
- Plot the demand per day using `ggplot`

```{r}
courses %>%
  filter(number < 100) %>%
  distinct(id, .keep_all = TRUE) %>%
  left_join(demand) %>%
  ggplot(aes(x = date, y = count, group = id)) +
  geom_line()
```

What's going on with those courses at the top? Let's take a look at the data we used above, and narrow in on only those courses that have that really high count towards the end. We can do this by looking only at our final day of enrollment, and filtering to include only those courses with enrollment greater than 75.

```{r}
courses %>%
  filter(number < 100) %>%
  distinct(id, .keep_all = TRUE) %>%
  left_join(demand) %>%
  filter(date == "1/24") %>%
  filter(count > 75)
```

Ah-ha! It's the Directed Studies courses messing up our data, which means that in the future, if we want to compute any averages on first-year seminars, we should also take care to filter out courses with `subject = "DRST"`.

### Computing ending enrollment per subject

As a final example, suppose we want to find out how many students ended up enrolling in each course, which we can assume is equal to the number of students for the final day of shopping period. The code below:

- Starts with the `courses` dataframe
- Merges in the demand data
- Makes the `date` column a R date object, instead of a string
- This allows us to then sort by `date`. Note that the rows are already sorted by date (because that's the way they were scraped), but this additional step ensures that it's in order *just in case* something happened
- Group by `code`. At this point, we have 20 rows per course code (one for each day of shopping period), so this command says tht all subsequent operations should be completed *per* group of rows
- For each course (for each group of rows), find:
  - The enrollment (count on the last day of shopping period), which is the last row in each group of rows
  - The peak demand, which is the maximum count within each group of rows
  - The peak demand day, which is the date corresponding to the day on which we saw maximum enrollment

```{r}
courses %>%
  left_join(demand) %>%
  mutate(date = as.Date(date, format = "%m/%d")) %>%
  arrange(code, date) %>%
  group_by(code) %>%
  summarize(enrollment = last(count),
            peak_demand = max(count),
            peak_demand_day = date[which.max(count)])
```

## Final comments

The point of the three examples above was to give you an idea of how to work your way around the dataset. It's a lot of data, and it might be overwhelming, but with a little bit of flexibility, we know you'll find something interesting.

One file that you can use as a starting point, or as a way to see some additional examples of working with the course demand data, is the code we used for the **Yale's Most Popular Courses**. You'll find a file named `create_table.R` in `/projects/yale-most-popular-courses/scripts`, which is the script we ran nightly to find the most up-to-date course demand patterns. If you think it'll be helpful, feel free to go through the script and see if you understand what's going on.

# Scraping Coursetable

## Getting the JSON file

Believe it or not, it's substantially easier to get the CourseTable data, since it doesn't need to be scraped! If you look at the source code for CourseTable by going to the website, right-clicking, and clicking "View Source" (not through Inspect Element), you'll find this code at the very bottom:

```
function jsonForSeason(season) {
    return '/GetDataFile.php?season=' + season;
}
```

Thus, in order to get the JSON file for a given semester, simply go to http://coursetable.com/GetDataFile.php?season= plus the season of the semester (YYYYSS, where SS = 03 for fall, 02 for summer, 01 for spring). This is the same season code as what we described above.

## Processing the JSON

We haven't done nearly as much working with the JSON yet. You can find a file called `clean_coursetable.py` that does *very* basic processing on the raw `coursetable.json` in Python, mostly to extract the only information that we needed (which was basic course info and meeting times), and output the `coursetable.csv` file. That code is pretty self-explanatory.

You can feel free to take the CourseTable data how you want: either continue working with it in Python, or translate it over to R and streamline the workflow in R.

# Merging the two

Anytime you're working with the data and you need to merge them, I recommend that you start with the following code, or some variation of it:

```{r}
# Libraries -----

library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)

# Helper functions -----

unite_code <- function(df) {
	tidyr::unite(df, code, subject, number, sep = " ", remove = FALSE)
}

separate_code <- function(df) {
	tidyr::separate(df, code, into = c("subject", "number"), sep = " ", remove = FALSE)
}

separate_designator <- function(df) {
	num <- df$number
	
	df$number <- as.numeric(str_extract(num, "[[:digit:]]+"))
	df$designator <- str_extract(num, "[:alpha:]")
	
	df
}

# Read in data -----

subjects <- read_csv("subjects.csv")

courses <- read_csv("courses.csv") %>%
	separate_code() %>%
	separate_designator() %>%
	select(id, code, subject, number, designator, name)

demand <- read_csv("demand.csv") %>%
	mutate(date = as.Date(date, format = "%m/%d")) %>%
  # merge in course data
	left_join(courses) %>%
	# getting rid of some duplicates: for some reason, certain obscure courses
  # show up more than once. these few lines are just manually getting rid of them
	arrange(code, date, desc(count)) %>%
	group_by(code, date) %>%
	slice(1L) %>%
	ungroup()

coursetable <- read_csv("coursetable.csv") %>%
	unite_code() %>%
	separate_designator() %>%
	select(code, subject, number, designator, section, times, locations, rating, workload)
```

(Modify the last chunk of `coursetable` code depending on how you choose to process it.)

The reasoning is because that all of the `unite_code()` and `separate_designator()` calls make the column format consistent across both the Course Demand Statistics and CourseTable data: each row of courses has one column for subject, number, and designator. This means you can more easily merge together the `courses` table and the `coursetable` table by using a `left_join()` call:

```{r}
left_join(demand, coursetable)
```

A few notes:

- CourseTable has distinct rows for courses with more than one section, while Course Demand Statistics does not. (Technically, you could scrape this too, but you'll have to write a more complex scraper to do so.) So we currently don't have a way to deal with cases where different sections have different ratings, other than simply taking the average of all instances of a course on CourseTable.
- There are quite a few instances of duplicate courses, and there are cases where the merging doesn't work properly, so when you're doing analysis, it's always a good idea to constantly be checking how many rows you're working with and how many rows you expect to be working with.
- There are also several cases where certain courses appear in CourseTable, but not in Course Demand Statistics, are vice versa, and we haven't settled upon a uniform way to deal with these.

# Existing visualizations

## Course rating and demand explorer

I completed this project as the final assignment for my Information and Data Visualization course (CPSC 446) last semester: [Yale Course Demand and CourseTable explorer](http://danielzhao.com/coursedemand/). The work I completed for this project forms the basis for a lot of what's in this guide.

I haven't gotten a chance to comment the code I used to process the data (and I was in a time crunch, so I wasn't doing it in the moment â€” bad programming practice!), but if you want to do more along the lines of what I did for this assignment, let me know and I'll get you a commented version of the code as soon as I can.

You can view the D3.js source just by right-clicking the page and going to "View Source".

## Yale's Most Popular Courses

This is the "big" project that we launched at the beginning of the semester. 

- The online page can be found at [Yale's Most Popular Courses](https://yaledailynews.com/blog/2020/01/10/yales-most-popular-courses/)
- All supporting files can be found at `/projects/yale-most-popular-courses/scripts`

You can view the D3.js source just by right-clicking the page and going to "View Source".

## Google Data Studio visualizations

Google Data Studio is an interactive data visualization tool much like Tableau. You simply drop in a `.csv` file (well, the interface is designed for more complex databases, which makes it a little cumbersome to use for basic `.csv` files). We've used it to prototype quick visualizations, and it might be helpful for you in that respect as well.

You can [use it for free](https://datastudio.google.com/overview) with your Google account.

**TODO**: add in a description about some of the GDS work that we've done so far.